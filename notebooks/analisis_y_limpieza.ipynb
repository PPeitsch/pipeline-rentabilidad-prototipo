{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY11WrpEqoGl"
   },
   "source": [
    "# Análisis Exploratorio y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOTAbfYWriw3"
   },
   "source": [
    "## 1. Configuración de Entorno y Carga de Datos\n",
    "\n",
    "Esta sección inicializa el entorno de trabajo, importando las librerías necesarias y configurando las rutas de los archivos. Se cargan los datos crudos desde un archivo CSV para comenzar el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZc1UHWypjl0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path to import our custom module\n",
    "# This makes the notebook runnable from the 'notebooks' directory\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "from data_cleaner import run_cleaning_pipeline\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Define relative file paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "input_file_path = os.path.join(project_root, 'data', 'rentabilidad_productos.csv')\n",
    "output_file_path = os.path.join(project_root, 'data', 'rentabilidad_productos_limpio.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurar que el notebook no falle si el archivo de datos no se encuentra, la carga se realiza dentro de un bloque `try-except`. La variable `df_raw` se inicializa como `None` para manejar este caso de forma controlada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t74GQZQqriZ"
   },
   "outputs": [],
   "source": [
    "# Initialize df_raw to None to handle potential FileNotFoundError\n",
    "df_raw = None\n",
    "try:\n",
    "    df_raw = pd.read_csv(input_file_path)\n",
    "    print(f\"DataFrame crudo cargado con {df_raw.shape[0]} filas y {df_raw.shape[1]} columnas.\")\n",
    "    display(df_raw.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo no se encontró en la ruta esperada: {input_file_path}\")\n",
    "    print(\"Por favor, asegúrese de que 'rentabilidad_productos.csv' exista en el directorio 'data'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75fx-KeWsKrL"
   },
   "source": [
    "## 2. Análisis Exploratorio de Datos Crudos (EDA)\n",
    "\n",
    "Antes de cualquier modificación, se realiza un análisis de los datos originales. El objetivo es entender su estructura, identificar problemas de calidad como valores nulos, duplicados o tipos de datos incorrectos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEpIrynjsuGO"
   },
   "source": [
    "### 2.1. Tipos de Datos y Resumen General\n",
    "\n",
    "El primer paso es obtener una visión general de la estructura del DataFrame. Se utiliza `info()` para revisar las columnas, el número de registros no nulos y, fundamentalmente, sus tipos de datos (`Dtype`), lo que permite detectar problemas obvios como números almacenados como texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHlYtznHs0f-"
   },
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy2TEtJzuZGj"
   },
   "source": [
    "### 2.2. Análisis de Valores Nulos\n",
    "\n",
    "Se cuantifican los valores nulos para cada columna y se visualiza su distribución con la librería `missingno`. Esto ayuda a identificar si los datos faltantes siguen algún patrón o son aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-es6YLXuT7N"
   },
   "outputs": [],
   "source": [
    "missing_values = df_raw.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_raw)) * 100\n",
    "missing_df = pd.DataFrame({'count': missing_values, 'percentage': missing_percentage})\n",
    "missing_df = missing_df[missing_df['count'] > 0].sort_values(by='count', ascending=False)\n",
    "\n",
    "print(f\"Se encontraron {len(missing_df)} columnas con valores nulos en los datos crudos.\")\n",
    "display(missing_df)\n",
    "\n",
    "msno.matrix(df_raw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmmuR0dfuhf-"
   },
   "source": [
    "### 2.3. Análisis de Duplicados en Clave Primaria (SKU)\n",
    "\n",
    "El `sku` debería ser un identificador único para cada producto. Se verifica si existen SKUs duplicados, lo cual indicaría un problema de integridad de datos. Para asegurar un conteo preciso, los nombres de columna se normalizan temporalmente (a minúsculas) antes de la verificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBL15PYRugq2"
   },
   "outputs": [],
   "source": [
    "# We need to normalize column names first to access 'sku' reliably\n",
    "temp_df = df_raw.copy()\n",
    "temp_df.columns = temp_df.columns.str.strip().str.lower()\n",
    "\n",
    "sku_counts = temp_df.sku.value_counts()\n",
    "duplicated_skus = sku_counts[sku_counts > 1]\n",
    "\n",
    "print(f\"Análisis de duplicados de SKU:\")\n",
    "print(f\"- Hay {len(sku_counts)} SKUs únicos de un total de {len(temp_df)} registros.\")\n",
    "print(f\"- Se encontraron {len(duplicated_skus)} SKUs con registros duplicados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx_8dH-lug67"
   },
   "source": [
    "## 3. Proceso de Limpieza de Datos\n",
    "\n",
    "Se invoca el pipeline de limpieza definido en el módulo `data_cleaner.py`. Esta función centralizada aplica todas las reglas de negocio y transformaciones descubiertas en el AED para producir un DataFrame limpio, manteniendo este notebook enfocado en el análisis y no en la implementación detallada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lgtnlv6vaTL"
   },
   "outputs": [],
   "source": [
    "df_cleaned = run_cleaning_pipeline(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBcDeFgHiJkL"
   },
   "source": [
    "## 4. Verificación y Análisis de Datos Limpios\n",
    "\n",
    "Una vez limpios los datos, es crucial verificar que el proceso fue exitoso y que los datos resultantes son coherentes. Se revisan los tipos de datos, la ausencia de nulos y la integridad de los valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmNoPqRsTuVw"
   },
   "source": [
    "### 4.1. Verificación de Tipos de Datos y Nulos\n",
    "\n",
    "Se repite la ejecución de `info()` y `isnull().sum()` sobre el DataFrame limpio. El objetivo es confirmar que las columnas numéricas y de fecha tienen los tipos correctos y que los valores nulos han sido tratados según la estrategia definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCxNzWbfwu2d"
   },
   "outputs": [],
   "source": [
    "print(\"--- Tipos de datos después de la limpieza ---\")\n",
    "df_cleaned.info()\n",
    "\n",
    "print(\"\\n--- Conteo de Nulos Restantes ---\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q3DEQvOvo-T"
   },
   "source": [
    "### 4.2. Verificación de Integridad de Datos (Cálculo de Margen)\n",
    "\n",
    "Esta es una validación de lógica de negocio: se calcula el margen de forma manual (`precio_venta` - `precio_compra`) y se compara con la columna `margen` original. Para evitar problemas con la precisión de los números de punto flotante, se utiliza una pequeña tolerancia de 0.01 en lugar de una comparación exacta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mnhY3sA1g88"
   },
   "outputs": [],
   "source": [
    "df_cleaned['margen_calculado'] = df_cleaned['precio_venta'] - df_cleaned['precio_compra']\n",
    "df_cleaned['margen_diferencia'] = (df_cleaned['margen'] - df_cleaned['margen_calculado']).abs()\n",
    "\n",
    "discrepancies = df_cleaned[df_cleaned['margen_diferencia'] > 0.01] # Use tolerance for float precision\n",
    "\n",
    "print(f\"Se encontraron {len(discrepancies)} filas con una diferencia mayor a 0.01 entre el margen reportado y el calculado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyZ123AbCdEf"
   },
   "source": [
    "### 4.3. Resumen Estadístico del DataFrame Limpio\n",
    "\n",
    "Se genera un resumen estadístico completo de todas las columnas del DataFrame limpio. Esto proporciona una visión cuantitativa de las distribuciones de datos (media, desviación estándar, cuartiles) y permite identificar posibles anomalías o outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azvap2mVwr1P"
   },
   "outputs": [],
   "source": [
    "display(df_cleaned.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2YEissO3jWD"
   },
   "source": [
    "### 4.4. Distribución de Variables Numéricas Limpias\n",
    "\n",
    "Para complementar el resumen estadístico, se visualizan las distribuciones de las columnas numéricas clave mediante histogramas y diagramas de caja (boxplots). Esto permite una inspección visual rápida de la forma de los datos, su simetría y la presencia de valores atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUPalz6U2WnC"
   },
   "outputs": [],
   "source": [
    "# Wrap in a check to ensure df_cleaned exists\n",
    "if 'df_cleaned' in locals() and df_cleaned is not None:\n",
    "    for col in ['precio_compra', 'precio_venta', 'margen']:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "        fig.suptitle(f\"Análisis de la columna numérica limpia: '{col}'\", fontsize=16)\n",
    "\n",
    "        sns.histplot(df_cleaned[col], kde=True, ax=axes[0])\n",
    "        axes[0].set_title('Distribución (Histograma)')\n",
    "\n",
    "        sns.boxplot(x=df_cleaned[col], ax=axes[1])\n",
    "        axes[1].set_title('Diagrama de Caja (Boxplot)')\n",
    "\n",
    "        # Use a tuple for the 'rect' parameter instead of a list\n",
    "        plt.tight_layout(rect=(0, 0.03, 1, 0.95))\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"El DataFrame 'df_cleaned' no está definido. Saltando la visualización.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Wh7AW7evprf"
   },
   "source": [
    "## 5. Almacenamiento del DataFrame Limpio\n",
    "\n",
    "El paso final es persistir el DataFrame limpio. Antes de guardar, se eliminan las columnas temporales creadas para la verificación (`margen_calculado`, `margen_diferencia`), ya que no son parte del conjunto de datos final. Se elige el formato Parquet por su eficiencia en almacenamiento y velocidad de lectura para futuros análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xc6m1KcvsdJ"
   },
   "outputs": [],
   "source": [
    "# Drop temporary columns used for verification before saving\n",
    "final_df_to_save = df_cleaned.drop(columns=['margen_calculado', 'margen_diferencia'])\n",
    "\n",
    "final_df_to_save.to_parquet(output_file_path, index=False, engine='pyarrow')\n",
    "\n",
    "print(f\"DataFrame limpio con {len(final_df_to_save)} filas guardado exitosamente en: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Nota\n",
    "\n",
    "Durante el proceso de limpieza, se tomaron dos decisiones clave que resultaron en la eliminación de filas del conjunto de datos original:\n",
    "\n",
    "1.  **Eliminación de SKUs duplicados:** Se eliminaron registros de productos con el mismo `sku`, conservando únicamente el que tenía la `fecha_actualizacion` más reciente. Esta es una estrategia común para obtener la \"versión más reciente\" de un registro, pero la decisión correcta depende estrictamente del negocio. En otros escenarios, podría ser necesario consolidar o promediar la información de los duplicados, o investigarlos como una anomalía en la fuente de datos.\n",
    "\n",
    "2.  **Eliminación de Nulos en Columnas Críticas:** Se descartaron filas que contenían valores nulos en columnas consideradas indispensables para el análisis, como `precio_compra`, `precio_venta` o `fecha_actualizacion`. Para este análisis, se asumió que un registro sin esta información carece de valor. Sin embargo, en un entorno de producción, una estrategia alternativa podría ser enviar estos registros a una \"cola de errores\" para una revisión manual o intentar enriquecer los datos faltantes desde otras fuentes antes de descartarlos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
